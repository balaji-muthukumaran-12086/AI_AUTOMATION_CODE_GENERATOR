"""
learning_agent.py
-----------------
Learning Agent: Analyzes batch test results, extracts actionable rules and
patterns using the LLM, updates the framework knowledge files, and drives
a hands-free heal â†’ re-run â†’ learn loop until all tests pass or retries
are exhausted.

What it learns
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  From FAILURES  â†’ DO / DON'T rules appended to config/framework_rules.md
  From SUCCESSES â†’ Working patterns appended to config/framework_knowledge.md
  All learnings  â†’ persisted to logs/learnings.jsonl (JSONL, one obj/line)

How CoderAgent consumes learnings
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  CoderAgent calls _load_recent_learnings() which reads the JSONL and injects
  the top-N entries as a "Recent Learnings" block in its system prompt.
  This means every future code generation benefits from past execution feedback.

Hands-free loop
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. run_batch â†’ batch_results
  2. LearningAgent.analyze(batch_results) â†’ extract learnings, update files
  3. For each failed result â†’ HealerAgent.heal(result) â†’ patch source
  4. ParallelRunnerAgent.run_batch(failed_configs) â†’ new results
  5. LearningAgent.analyze(new_results) â†’ more learnings
  6. Repeat up to LEARNING_RETRIES times
  7. Final summary stored in state

Pipeline position:
  ... â†’ parallel_runner â†’ learning â†’ [heal_loop?] â†’ END
"""

import json
import re
import threading
from datetime import datetime
from pathlib import Path
from typing import Optional

from langchain_core.messages import SystemMessage, HumanMessage

from agents.state import AgentState
from agents.llm_factory import get_llm
from config.project_config import (
    BASE_DIR,
    DEPS_DIR,
    PROJECT_NAME,
    LEARNINGS_LOG_PATH,
    LEARNING_TOP_N,
    LEARNING_RETRIES,
)


# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_RULES_FILE       = Path(BASE_DIR) / "config" / "framework_rules.md"
_KNOWLEDGE_FILE   = Path(BASE_DIR) / "config" / "framework_knowledge.md"
_LEARNINGS_LOG    = Path(LEARNINGS_LOG_PATH)
_LEARNED_RULES_HEADER    = "\n\n---\n\n## LEARNED RULES (auto-generated by LearningAgent)\n"
_LEARNED_PATTERNS_HEADER = "\n\n---\n\n## LEARNED PATTERNS (auto-generated by LearningAgent)\n"

# File-level write lock (protects knowledge files from concurrent writes)
_file_lock = threading.Lock()


# â”€â”€ Public helper for CoderAgent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_recent_learnings(top_n: int = LEARNING_TOP_N) -> str:
    """
    Return the last `top_n` learnings as a formatted string for injection
    into agent prompts.  Returns empty string if no learnings yet.
    """
    if not _LEARNINGS_LOG.exists():
        return ""
    lines = _LEARNINGS_LOG.read_text(encoding="utf-8").strip().splitlines()
    entries = []
    for line in reversed(lines):
        line = line.strip()
        if line:
            try:
                entries.append(json.loads(line))
            except Exception:
                pass
        if len(entries) >= top_n:
            break

    if not entries:
        return ""

    parts = ["=== Recent Learnings from Past Executions ==="]
    for i, e in enumerate(entries, 1):
        ltype = e.get("learning_type", "INFO")
        title = e.get("title", "")
        body  = e.get("body", e.get("description", ""))
        src   = e.get("source", "")
        parts.append(f"{i}. [{ltype}] {title}")
        if body:
            parts.append(f"   {body}")
        if src:
            parts.append(f"   Source: {src}")
    return "\n".join(parts)


# â”€â”€ Learning Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LearningAgent:
    """
    Extracts structured learnings from batch run results and feeds them
    back into framework_rules.md, framework_knowledge.md, and
    logs/learnings.jsonl.  Also drives the hands-free healâ†’re-run loop.
    """

    def __init__(self, base_dir: str = None, deps_dir: str = None):
        self.base     = Path(base_dir) if base_dir else Path(BASE_DIR)
        self.deps_dir = Path(deps_dir) if deps_dir else Path(DEPS_DIR)
        self.llm      = get_llm(temperature=0.1)
        _LEARNINGS_LOG.parent.mkdir(parents=True, exist_ok=True)

    # â”€â”€ LangGraph node entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self, state: AgentState) -> AgentState:
        """
        LangGraph node.

        Reads:   state['batch_run_results'], state['batch_run_configs']
        Writes:  state['learnings'], state['batch_run_results'] (updated after re-runs),
                 state['messages'], state['learning_iteration']
        """
        batch_results = state.get("batch_run_results", [])
        batch_configs = state.get("batch_run_configs") or self._configs_from_results(batch_results)
        iteration     = state.get("learning_iteration", 0)

        if not batch_results:
            state["messages"] = ["[LearningAgent] No batch results to learn from."]
            return state

        print(f"[LearningAgent] ðŸ§  Analyzing {len(batch_results)} result(s) "
              f"(iteration {iteration + 1})...")

        # â”€â”€ Extract learnings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        new_learnings = self.analyze_batch(batch_results)

        # â”€â”€ Persist & update files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self._persist_learnings(new_learnings)
        rules    = [l for l in new_learnings if l.get("learning_type") == "RULE"]
        patterns = [l for l in new_learnings if l.get("learning_type") == "PATTERN"]
        if rules:
            self._append_to_rules_file(rules)
        if patterns:
            self._append_to_knowledge_file(patterns)

        fails = [r for r in batch_results if not r.get("success", False)]
        print(f"[LearningAgent] Extracted {len(new_learnings)} learnings. "
              f"Failures: {len(fails)}/{len(batch_results)}")

        # â”€â”€ Hands-free heal + re-run loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        final_results = list(batch_results)
        all_learnings = list(new_learnings)

        if fails and iteration < LEARNING_RETRIES:
            print(f"[LearningAgent] ðŸ”„ Hands-free loop: "
                  f"healing {len(fails)} failure(s) and re-running...")
            failed_configs = [
                c for c in batch_configs
                if any(
                    r.get("entity_class") == c.get("entity_class")
                    and r.get("method_name") == c.get("method_name")
                    for r in fails
                )
            ]
            healed_results = self._heal_and_rerun(fails, failed_configs)
            # Merge: replace failed results with new ones
            merged = {(r["entity_class"], r["method_name"]): r for r in final_results}
            for r in healed_results:
                merged[(r["entity_class"], r["method_name"])] = r
            final_results = list(merged.values())

            # Learn from re-run results too
            rerun_learnings = self.analyze_batch(healed_results)
            self._persist_learnings(rerun_learnings)
            all_learnings.extend(rerun_learnings)
            rerun_rules = [l for l in rerun_learnings if l.get("learning_type") == "RULE"]
            rerun_pats  = [l for l in rerun_learnings if l.get("learning_type") == "PATTERN"]
            if rerun_rules:
                self._append_to_rules_file(rerun_rules)
            if rerun_pats:
                self._append_to_knowledge_file(rerun_pats)

        passed = sum(1 for r in final_results if r.get("success"))
        summary = (
            f"[LearningAgent] Final: {passed}/{len(final_results)} passed. "
            f"Learnings extracted: {len(all_learnings)}. "
            f"Rules added: {len([l for l in all_learnings if l.get('learning_type')=='RULE'])}. "
            f"Patterns added: {len([l for l in all_learnings if l.get('learning_type')=='PATTERN'])}."
        )
        print(summary)

        state["learnings"]          = all_learnings
        state["batch_run_results"]  = final_results
        state["learning_iteration"] = iteration + 1
        state["messages"]           = [summary]
        return state

    # â”€â”€ Core analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def analyze_batch(self, results: list[dict]) -> list[dict]:
        """
        Analyze a list of RunResult dicts and return a list of learning dicts.
        Runs LLM calls concurrently (one per result).
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed as _ac

        learnings = []
        with ThreadPoolExecutor(max_workers=4) as pool:
            futures = {}
            for r in results:
                if r.get("success"):
                    futures[pool.submit(self._extract_success_pattern, r)] = r
                else:
                    futures[pool.submit(self._extract_failure_rule, r)] = r

            for future in _ac(futures):
                result = futures[future]
                try:
                    learning = future.result()
                    if learning:
                        learnings.append(learning)
                except Exception as exc:
                    print(f"[LearningAgent] LLM analysis error for "
                          f"{result.get('entity_class')}.{result.get('method_name')}: {exc}")
        return learnings

    # â”€â”€ LLM: extract failure rule â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _extract_failure_rule(self, result: dict) -> Optional[dict]:
        """
        Analyze a failed run result and extract a DO/DON'T rule using the LLM.
        Returns a learning dict or None.
        """
        entity_class = result.get("entity_class", "?")
        method_name  = result.get("method_name", "?")
        stdout       = result.get("stdout", "")
        stderr       = result.get("stderr", "")
        error        = result.get("error", "")

        # Read source for context
        source_snippet = self._read_entity_source(entity_class, max_chars=3000)

        prompt = f"""You are an expert in the AutomaterSelenium Java test framework for Zoho ServiceDesk Plus.

A test FAILED:
  Entity: {entity_class}
  Method: {method_name}

ERROR (key lines):
{self._extract_key_error_lines(stdout + error + stderr)}

FAILING SOURCE CONTEXT:
{source_snippet}

Analyze the failure and produce ONE actionable rule to prevent this category of error in future tests.

Return a JSON object with these EXACT keys:
{{
  "title":        "Short rule title (max 10 words)",
  "section":      "Which framework_rules.md section this belongs to (e.g. SECTION 4 â€” API CALLS)",
  "rule_type":    "DON'T" or "DO",
  "body":         "Clear explanation of the rule (2-4 sentences)",
  "bad_example":  "Java code showing the WRONG pattern (single code line or null if N/A)",
  "good_example": "Java code showing the CORRECT pattern (single code line or null if N/A)",
  "source":       "{entity_class}.{method_name}"
}}

RULES for your response:
- Focus on the ROOT CAUSE pattern, not the specific test details
- The rule must be GENERALISABLE â€” applicable to any entity, not just {entity_class}
- Return ONLY valid JSON, no markdown"""

        try:
            response = self.llm.invoke([
                SystemMessage(content="You are a Java test automation rule extractor. Return only valid JSON."),
                HumanMessage(content=prompt),
            ])
            text = re.sub(r"```json\s*|\s*```", "", response.content.strip()).strip()
            data = json.loads(text)
            data["learning_type"] = "RULE"
            data["timestamp"]     = datetime.now().isoformat()
            return data
        except Exception as exc:
            print(f"[LearningAgent] Rule extraction LLM error: {exc}")
            return None

    # â”€â”€ LLM: extract success pattern â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _extract_success_pattern(self, result: dict) -> Optional[dict]:
        """
        Analyze a passing run result and extract a reusable pattern using the LLM.
        Returns a learning dict or None.
        """
        entity_class = result.get("entity_class", "?")
        method_name  = result.get("method_name", "?")

        # Read the specific passing method body
        method_body = self._read_method_body(entity_class, method_name)
        if not method_body:
            return None  # nothing useful to analyse

        prompt = f"""You are an expert in the AutomaterSelenium Java test framework for Zoho ServiceDesk Plus.

A test PASSED:
  Entity: {entity_class}
  Method: {method_name}

PASSING METHOD BODY:
{method_body}

Extract ONE transferable coding pattern that made this test succeed and that other test writers should reuse.

Return a JSON object with these EXACT keys:
{{
  "title":        "Short pattern name (max 10 words)",
  "description":  "What this pattern does and why it works (2-3 sentences)",
  "code_example": "The key Java code block illustrating this pattern (copy from source)",
  "applies_to":   "Which scenario types benefit from this pattern (1 sentence)",
  "source":       "{entity_class}.{method_name}"
}}

RULES:
- Focus on framework-level patterns (how to call preProcess, how to use LocalStorage,
  how to handle checkboxes, how to validate success messages, etc.)
- Make the title and description generic â€” not specific to solutions/requests etc.
- Return ONLY valid JSON, no markdown"""

        try:
            response = self.llm.invoke([
                SystemMessage(content="You are a Java test pattern extractor. Return only valid JSON."),
                HumanMessage(content=prompt),
            ])
            text = re.sub(r"```json\s*|\s*```", "", response.content.strip()).strip()
            data = json.loads(text)
            data["learning_type"] = "PATTERN"
            data["timestamp"]     = datetime.now().isoformat()
            return data
        except Exception as exc:
            print(f"[LearningAgent] Pattern extraction LLM error: {exc}")
            return None

    # â”€â”€ File updaters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _append_to_rules_file(self, rules: list[dict]) -> None:
        """Append extracted DO/DON'T rules to framework_rules.md."""
        lines = []
        for r in rules:
            title    = r.get("title", "Unnamed Rule")
            section  = r.get("section", "GENERAL")
            rtype    = r.get("rule_type", "DO")
            body     = r.get("body", "")
            bad      = r.get("bad_example")
            good     = r.get("good_example")
            src      = r.get("source", "")
            ts       = r.get("timestamp", datetime.now().isoformat())[:10]

            lines.append(f"\n### [{rtype}] {title}")
            lines.append(f"_Section: {section} | Learned from: {src} | Date: {ts}_\n")
            lines.append(body)
            if bad:
                lines.append(f"\n```java\n// âŒ WRONG\n{bad}\n```")
            if good:
                lines.append(f"\n```java\n// âœ… CORRECT\n{good}\n```")
        self._write_to_file(_RULES_FILE, _LEARNED_RULES_HEADER, "\n".join(lines))
        print(f"[LearningAgent] âœ… {len(rules)} rule(s) appended to framework_rules.md")

    def _append_to_knowledge_file(self, patterns: list[dict]) -> None:
        """Append extracted patterns to framework_knowledge.md."""
        lines = []
        for p in patterns:
            title   = p.get("title", "Unnamed Pattern")
            desc    = p.get("description", "")
            code    = p.get("code_example", "")
            applies = p.get("applies_to", "")
            src     = p.get("source", "")
            ts      = p.get("timestamp", datetime.now().isoformat())[:10]

            lines.append(f"\n### {title}")
            lines.append(f"_Learned from: {src} | Date: {ts}_\n")
            lines.append(desc)
            if applies:
                lines.append(f"\n**Applies to:** {applies}")
            if code:
                lines.append(f"\n```java\n{code}\n```")
        self._write_to_file(_KNOWLEDGE_FILE, _LEARNED_PATTERNS_HEADER, "\n".join(lines))
        print(f"[LearningAgent] âœ… {len(patterns)} pattern(s) appended to framework_knowledge.md")

    def _write_to_file(self, path: Path, section_header: str, content: str) -> None:
        """
        Append content to a knowledge file under a dedicated auto-generated section.
        Thread-safe: uses _file_lock.
        """
        with _file_lock:
            existing = path.read_text(encoding="utf-8") if path.exists() else ""
            if section_header.strip() not in existing:
                # First time â€” add the section heading
                new_content = existing + section_header + content
            else:
                # Section already exists â€” append to end of file
                new_content = existing + "\n" + content
            path.write_text(new_content, encoding="utf-8")

    def _persist_learnings(self, learnings: list[dict]) -> None:
        """Append learnings to logs/learnings.jsonl (one JSON object per line)."""
        with _file_lock:
            with open(_LEARNINGS_LOG, "a", encoding="utf-8") as f:
                for l in learnings:
                    f.write(json.dumps(l, ensure_ascii=False) + "\n")

    # â”€â”€ Hands-free heal + re-run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _heal_and_rerun(self, failed_results: list[dict], failed_configs: list[dict]) -> list[dict]:
        """
        For each failed result: invoke HealerAgent, then re-run via ParallelRunnerAgent.
        Returns a list of new RunResult dicts.
        """
        from agents.healer_agent import HealerAgent
        from agents.parallel_runner_agent import ParallelRunnerAgent

        healer = HealerAgent(base_dir=str(self.base), deps_dir=str(self.deps_dir))
        rerun_configs = []

        for result in failed_results:
            entity_class = result.get("entity_class", "")
            method_name  = result.get("method_name", "")
            # Find matching config
            cfg = next(
                (c for c in failed_configs
                 if c.get("entity_class") == entity_class
                 and c.get("method_name") == method_name),
                None,
            )
            if cfg is None:
                continue

            print(f"[LearningAgent] ðŸ©º Healing: {entity_class}.{method_name}")
            try:
                heal_result = healer.heal_from_run_result(
                    run_result=result,
                    run_config=cfg,
                )
                if heal_result.healed:
                    print(f"[LearningAgent] âœ… Healed â€” queuing re-run: {method_name}")
                    rerun_configs.append(cfg)
                else:
                    print(f"[LearningAgent] âš ï¸  Could not heal {method_name}: {heal_result.error}")
            except Exception as exc:
                print(f"[LearningAgent] Healer error for {method_name}: {exc}")

        if not rerun_configs:
            print("[LearningAgent] No tests were healed â€” skipping re-run.")
            return failed_results  # return original failed results unchanged

        print(f"[LearningAgent] Re-running {len(rerun_configs)} healed test(s)...")
        runner = ParallelRunnerAgent(base_dir=str(self.base), deps_dir=str(self.deps_dir))
        batch = runner.run_batch(rerun_configs)
        print(batch.summary())
        return batch.to_list()

    # â”€â”€ Source helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _read_entity_source(self, entity_class: str, max_chars: int = 3000) -> str:
        """Find and return the Base source file for an entity class (trimmed)."""
        src_root = self.base / PROJECT_NAME / "src"
        for candidate in src_root.rglob(f"{entity_class}Base.java"):
            content = candidate.read_text(encoding="utf-8")
            return content[:max_chars]
        for candidate in src_root.rglob(f"{entity_class}.java"):
            content = candidate.read_text(encoding="utf-8")
            return content[:max_chars]
        return ""

    def _read_method_body(self, entity_class: str, method_name: str, max_chars: int = 3000) -> str:
        """
        Extract the body of a specific method from the entity source file.
        Returns empty string if not found.
        """
        source = self._read_entity_source(entity_class, max_chars=50000)
        if not source:
            return ""
        # Find method start â€” matches `public void methodName(` or `public <type> methodName(`
        pattern = re.compile(
            rf'public\s+\w[\w<>]*\s+{re.escape(method_name)}\s*\(',
        )
        match = pattern.search(source)
        if not match:
            return ""
        start = match.start()
        # Walk forward to find matching closing brace
        depth = 0
        i = start
        while i < len(source):
            if source[i] == '{':
                depth += 1
            elif source[i] == '}':
                depth -= 1
                if depth == 0:
                    return source[start:i + 1][:max_chars]
            i += 1
        return source[start:start + max_chars]

    def _extract_key_error_lines(self, text: str, max_lines: int = 20) -> str:
        """Extract the most relevant error lines from combined output."""
        relevant = []
        for line in text.splitlines():
            if any(kw in line for kw in [
                "Exception", "Error", "FAILED", "addFailureReport",
                "Caused by", "NoSuchElement", "TimeoutException",
                "NullPointer", "Unable to locate", "$$Failure",
                "cannot find symbol",
            ]):
                relevant.append(line)
        return "\n".join(relevant[:max_lines]) if relevant else text[-1000:]

    def _configs_from_results(self, results: list[dict]) -> list[dict]:
        """Build minimal run_config dicts from RunResult dicts (for re-run)."""
        from config.project_config import SDP_URL, SDP_PORTAL, SDP_ADMIN_EMAIL
        return [
            {
                "entity_class":  r.get("entity_class", ""),
                "method_name":   r.get("method_name", ""),
                "url":           r.get("url", SDP_URL),
                "admin_mail_id": SDP_ADMIN_EMAIL,
                "email_id":      SDP_ADMIN_EMAIL,
                "portal_name":   SDP_PORTAL,
                "skip_compile":  True,
            }
            for r in results
        ]
