# AI Automation QA â€” Pipeline Flow & Architecture

> **Last updated:** February 27, 2026  
> **Active model:** `qwen2.5-coder:7b` via Ollama (localhost:11434)  
> **Entry point:** Web UI at `http://localhost:9500`

---

## Overview

The pipeline is a **LangGraph state machine** with 9 agents chained in sequence.  
Each agent reads from a shared `AgentState` dict, does its work, writes results back, and passes control to the next agent.

Out of 9 agents, **5 call Ollama** (the local LLM) and **4 do not** â€” they use rule-based logic, file I/O, or subprocess calls instead.

---

## Flow Diagram

```mermaid
flowchart TD
    A([ðŸ–¥ï¸ User submits feature\nvia Web UI]) --> B

    B[ðŸ“„ IngestionAgent\nParse uploaded doc if any\nNo LLM] --> C

    C[ðŸ“ PlannerAgent\nðŸ¤– Ollama Call 1\nMap feature â†’ module paths\n+ scenario list] --> D

    D[ðŸ” CoverageAgent\nDeduplicate against ChromaDB\nNo LLM] --> E

    E[ðŸ•µï¸ UIScoutAgent\nðŸ¤– Ollama Call 2 + 3\nNavigate live SDP UI\nCapture DOM observations] --> F

    F[âš™ï¸ CoderAgent\nðŸ¤– Ollama Call 4\nRAG context from ChromaDB\nGenerate Java test code] --> G

    G[ðŸ”Ž ReviewerAgent\nðŸ¤– Ollama Call 5\nStatic + semantic review] --> H{Needs\nrevision?}

    H -->|Yes â€” max 2 times| F
    H -->|No / limit hit| I

    I[ðŸ’¾ OutputAgent\nWrite .java snippet files\nNo LLM] --> J{run_config\nsupplied?}

    J -->|No| M
    J -->|Yes| K

    K[ðŸ§ª RunnerAgent\njavac compile + run test\nNo LLM] --> L{Test\npassed?}

    L -->|Yes| M
    L -->|No| N

    N[ðŸ©º HealerAgent\nðŸ¤– Ollama Call 6 + 7\nClassify failure type\nDerive locator or code fix] --> M

    M[ðŸ”€ HgAgent\nhg branch + commit\nif enabled â€” No LLM] --> O

    O([âœ… Done\nFiles shown in Web UI])

    style B  fill:#d4edda,stroke:#28a745,color:#000
    style D  fill:#d4edda,stroke:#28a745,color:#000
    style I  fill:#d4edda,stroke:#28a745,color:#000
    style K  fill:#d4edda,stroke:#28a745,color:#000
    style M  fill:#d4edda,stroke:#28a745,color:#000
    style C  fill:#fff3cd,stroke:#e6a817,color:#000
    style E  fill:#fff3cd,stroke:#e6a817,color:#000
    style F  fill:#fff3cd,stroke:#e6a817,color:#000
    style G  fill:#fff3cd,stroke:#e6a817,color:#000
    style N  fill:#fff3cd,stroke:#e6a817,color:#000
    style H  fill:#cce5ff,stroke:#004085,color:#000
    style J  fill:#cce5ff,stroke:#004085,color:#000
    style L  fill:#cce5ff,stroke:#004085,color:#000
    style A  fill:#e2d9f3,stroke:#6f42c1,color:#000
    style O  fill:#c8e6c9,stroke:#2e7d32,color:#000
```

**Legend**

| Colour | Meaning |
|--------|---------|
| ðŸŸ¡ Yellow | Agent calls Ollama (LLM) |
| ðŸŸ¢ Green | Agent uses rule-based logic / file I/O / subprocess â€” no LLM |
| ðŸ”µ Blue | Decision / routing point |
| ðŸŸ£ Purple | External entry / exit |

---

## Agent-by-Agent Explanation

### 1 Â· IngestionAgent â€” `agents/ingestion_agent.py`
**No LLM**

**What it does:**  
If the user uploaded a document (PDF, DOCX, XLSX, TXT) alongside the feature description, this agent parses it and extracts structured use-cases, enriching the feature description and suggesting target modules.  

If no document was uploaded (text-only flow), it is a **no-op** â€” it immediately passes control to the next agent.

**Input â†’ Output:**
- Input: `source_document` path (optional)
- Output: enriched `feature_description`, populated `target_modules`

---

### 2 Â· PlannerAgent â€” `agents/planner_agent.py`
**ðŸ¤– Ollama Call #1**

**What it does:**  
This is the first brain of the pipeline. It takes the feature description and the SDP ITSM module taxonomy (`config/module_taxonomy.yaml`) and asks the LLM to figure out:
- Which module path(s) the feature belongs to (e.g., `modules/requests/request/`)
- What specific test scenarios should be generated (create, edit, delete, validate, etc.)

**What Ollama is asked:**  
> *"Given this ITSM feature description and the available module taxonomy, produce a JSON test plan: `{ "module/path": [{ "description": ..., "type": ..., "priority": ... }] }`"*

**Input â†’ Output:**
- Input: `feature_description`, `target_modules` (optional hint)
- Output: `test_plan` â€” a dict mapping each module path to a list of scenario specs

**Why it's critical:**  
Everything downstream depends on this. If the LLM produces an empty or wrong `test_plan`, no code gets generated. This is also the most memory-intensive step â€” Ollama needs **4.3 GiB RAM** loaded to answer.

---

### 3 Â· CoverageAgent â€” `agents/coverage_agent.py`
**No LLM â€” uses ChromaDB vector search**

**What it does:**  
Before generating code for a scenario, this agent queries the ChromaDB knowledge base (14,637 vectors from 17,101 existing scenarios across 210 modules) to check if a similar test already exists.

- If a scenario is **too similar** to an existing one â†’ it is marked as a duplicate and skipped
- If it is **genuinely new** â†’ it passes through as a coverage gap

This prevents generating redundant tests that are already in the codebase.

**Input â†’ Output:**
- Input: `test_plan` from PlannerAgent
- Output: `coverage_gaps` (new scenarios), `duplicate_warnings`, filtered `proposed_scenarios`

---

### 4 Â· UIScoutAgent â€” `agents/ui_scout_agent.py`
**ðŸ¤– Ollama Call #2 + #3**

**What it does:**  
Before writing any code, the agent uses **Playwright** to open a real browser, navigate to the actual SDP application, and observe the live UI for each target module.

It captures an **accessibility snapshot** (DOM tree of buttons, inputs, labels, and their states) from the real running SDP instance.

**Two Ollama calls are made here:**

- **Call #2 â€” Scout plan inference:**  
  For modules with no pre-defined navigation plan, asks Ollama:  
  > *"For the module path `modules/requests/request/`, what are the step-by-step UI navigation steps to reach the form?"*

- **Call #3 â€” DOM annotation:**  
  After Playwright captures the snapshot, asks Ollama:  
  > *"Here is the DOM snapshot of the page. What are the actionable UI observations â€” which fields are visible, which buttons trigger what, what state changes happen?"*

**Input â†’ Output:**
- Input: `proposed_scenarios` (to know which modules to scout)
- Output: `ui_observations` â€” a dict of `{ module_path: [UIObservation] }` injected into the Coder's prompt

**Why it matters:**  
This gives the CoderAgent live ground truth about the UI instead of relying purely on outdated examples from the knowledge base.

---

### 5 Â· CoderAgent â€” `agents/coder_agent.py`
**ðŸ¤– Ollama Call #4 (once per module)**

**What it does:**  
This is the primary code generation step. For each module in the `test_plan`, it:

1. **Retrieves RAG context** from ChromaDB â€” fetches the 3 most similar existing test implementations as examples
2. **Fetches help guide snippets** â€” field definitions, locator patterns specific to the module
3. **Injects the UI observations** from UIScoutAgent as additional context
4. **Asks Ollama** to generate the Java `@AutomaterScenario` method(s)

**What Ollama is asked:**  
> *"Here are framework rules, similar existing tests, live UI observations, and the scenarios to implement. Generate the Java `@AutomaterScenario` annotated method and the corresponding `Base.java` logic."*

The output must follow a **two-piece format**:
```
// ===== ADD TO: IncidentRequestBase.java =====
<method body>

// ===== ADD TO: IncidentRequest.java =====
<@AutomaterScenario wrapper>
```

**Input â†’ Output:**
- Input: `test_plan`, `ui_observations`
- Output: `generated_code` list â€” one entry per module with the raw Java code string

---

### 6 Â· ReviewerAgent â€” `agents/reviewer_agent.py`
**ðŸ¤– Ollama Call #5 (once per module)**

**What it does:**  
Runs in two phases for the generated code:

**Phase A â€” Static checks (no LLM):**
- Are required imports present (`EntityCase`, `SDPCloudActions`, etc.)?
- Is `@AutomaterScenario` annotation present with required fields?
- Do method signatures match the framework conventions?
- Are data key references valid?

**Phase B â€” Semantic review (Ollama):**  
> *"Does this code correctly implement the described scenario? Does it follow the framework patterns? Are there logic errors or missing steps?"*

If issues are found, it populates `revision_requests` with specific fix instructions.

**The revision loop:**  
If `revision_requests` is non-empty AND `revision_count < 2`, the pipeline **routes back to CoderAgent** with the fix notes injected into the prompt. This gives the LLM up to 2 chances to self-correct before proceeding regardless.

**Input â†’ Output:**
- Input: `generated_code`
- Output: `review_results`, `revision_requests`

---

### 7 Â· OutputAgent â€” `agents/output_agent.py`
**No LLM â€” file I/O**

**What it does:**  
Parses the two-piece format from CoderAgent's output and writes the Java snippet files to disk under a timestamped run directory:

```
generated/
  <timestamp>_<module_name>/
    IncidentRequestBase_snippet.java
    IncidentRequest_snippet.java
    summary.json
```

Also collects `generation_instructions` â€” copy-paste-ready terminal commands telling the developer which files to patch in the real project.

**Input â†’ Output:**
- Input: `generated_code`, `review_results`
- Output: `final_output_paths`, `generated_dir`, `generation_instructions`

---

### 8 Â· RunnerAgent â€” `agents/runner_agent.py`
**No LLM â€” Java subprocess**  
**Only activated if `run_config` is provided**

**What it does:**  
Compiles and executes the generated test against the live SDP instance:

1. `javac` targeted compile of just the two patched files
2. Patches `StandaloneDefault.java` with the test class and method
3. Runs `run_test.py` as a subprocess
4. Captures stdout, parses for `$$Failure` / `"successfully"` / exceptions
5. Returns `run_result { success: bool, stdout: str, summary: str }`

**Input â†’ Output:**
- Input: `run_config` (url, email, entity_class, method_name)
- Output: `run_result`

---

### 9 Â· HealerAgent â€” `agents/healer_agent.py`
**ðŸ¤– Ollama Call #6 + #7**  
**Only activated if RunnerAgent ran AND test failed**

**What it does:**  
Attempts to automatically fix the failing test without human intervention.

**Call #6 â€” Failure classification:**  
> *"The test output is: `[stdout]`. Is this a LOCATOR failure, API failure, LOGIC failure, or OTHER?"*

**Call #7 â€” Fix derivation:**
- **LOCATOR failure:** Playwright navigates to the failing page, captures a fresh accessibility snapshot, then asks Ollama:  
  > *"The old XPath `//button[@id='save']` no longer works. Here is the current DOM. What is the correct XPath or `By` locator?"*
- **API / LOGIC failure:** Asks Ollama to generate a code patch based on the error output.

After deriving the fix, Healer patches the Java source, recompiles, and reruns. The result is stored in `heal_result`.

---

### 10 Â· HgAgent â€” `agents/hg_agent.py`
**No LLM â€” VCS operations**  
**Gated by `HG_AGENT_ENABLED` flag (currently `False`)**

**What it does:**  
When enabled, creates a Mercurial branch named after the run and commits all generated files. This gives every auto-generated test its own traceable branch in the SDP source repository.

---

## Where Ollama Is Used â€” Summary

| Ollama Call | Agent | Prompt Purpose | When skipped |
|-------------|-------|---------------|--------------|
| **#1** | PlannerAgent | Feature â†’ JSON test plan | Never (always runs) |
| **#2** | UIScoutAgent | Infer navigation plan for unknown module | Module has a pre-built scout plan |
| **#3** | UIScoutAgent | DOM snapshot â†’ actionable UI observations | Playwright fails to reach page |
| **#4** | CoderAgent | Generate Java `@AutomaterScenario` code | If test_plan is empty (e.g. OOM in #1) |
| **#5** | ReviewerAgent | Semantic code review | If generated_code is empty |
| **#6** | HealerAgent | Classify failure type | Test passed, or RunnerAgent not run |
| **#7** | HealerAgent | Derive XPath fix or code patch | Test passed, or failure is OTHER type |

**Maximum LLM calls in a full run: 7**  
**Minimum LLM calls (code-gen only, happy path): 3** (Planner + Coder + Reviewer)

---

## State Flow Between Agents

```
AgentState (shared dict)
â”‚
â”œâ”€â”€ feature_description     â† set by user / enriched by IngestionAgent
â”œâ”€â”€ test_plan               â† written by PlannerAgent
â”œâ”€â”€ coverage_gaps           â† written by CoverageAgent
â”œâ”€â”€ ui_observations         â† written by UIScoutAgent
â”œâ”€â”€ generated_code          â† written by CoderAgent
â”œâ”€â”€ review_results          â† written by ReviewerAgent
â”œâ”€â”€ revision_requests       â† written by ReviewerAgent (triggers loop back)
â”œâ”€â”€ revision_count          â† incremented by pipeline router
â”œâ”€â”€ final_output_paths      â† written by OutputAgent
â”œâ”€â”€ generation_instructions â† written by OutputAgent
â”œâ”€â”€ run_result              â† written by RunnerAgent
â”œâ”€â”€ heal_result             â† written by HealerAgent
â”œâ”€â”€ hg_result               â† written by HgAgent
â”œâ”€â”€ messages                â† appended by every agent (streamed to UI)
â””â”€â”€ errors                  â† appended on failure (streamed to UI)
```

> All `messages` and `errors` fields use `Annotated[list[str], operator.add]` â€” LangGraph's reducer automatically accumulates entries across nodes. Each agent only needs to return the **new** messages it wants to add.

---

## External Dependencies

| Service | Role | Used by |
|---------|------|---------|
| **Ollama** `localhost:11434` | Local LLM inference (`qwen2.5-coder:7b`, 4.3 GiB) | Planner, UIScout, Coder, Reviewer, Healer |
| **ChromaDB** `knowledge_base/chroma_db/` | Vector similarity search â€” 14,637 scenario vectors | Coverage, Coder |
| **Playwright** (Chromium) | Live browser for UI observation and self-healing | UIScout, Healer |
| **SDP Application** `sdpodqa-auto1.csez.zohocorpin.com:9090` | Target app for UI scouting and test execution | UIScout, Runner, Healer |
| **Mercurial** `hg` | Source control for generated tests | HgAgent (gated off) |

---

## Known Constraints

| Constraint | Detail |
|------------|--------|
| **RAM** | Ollama needs 4.3 GiB free to load `qwen2.5-coder:7b`. If available RAM < 4.3 GiB, PlannerAgent fails with OOM and the pipeline produces no output. |
| **Speed** | All inference is CPU-only. A full pipeline run takes **15â€“25 minutes** on this machine. |
| **Revision cap** | The Reviewer â†’ Coder loop is capped at **2 revisions** to prevent infinite loops. After 2 cycles, whatever code exists is passed to OutputAgent regardless. |
| **Runner opt-in** | RunnerAgent only activates if `run_config` is included in the initial state. The Web UI does not send `run_config` by default â€” test execution is a separate step. |
| **HgAgent gated** | `HG_AGENT_ENABLED = False` in `config/project_config.py`. Set to `True` to enable auto-branch + commit. |
